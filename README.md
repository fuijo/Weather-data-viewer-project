# Weather_data_viewer
# Project Overview: 
The Weather Data Project is an interactive web-based application designed to visualize and analyze weather data across global cities. It integrates data-driven features, map-based visualizations, and an intuitive user interface to provide insights into geographic weather patterns.
![image](https://github.com/user-attachments/assets/694ccd51-810c-443f-a4a4-9e6b00b1050f)

** Data Visualization and Data Engineering Track Project** 
**Project Overview:** The goal of this group project was to develop an interactive data visualization application, utilizing large datasets and advanced techniques, and to manage and engineer the data for meaningful analysis and presentation. The project included the creation of visualizations and the implementation of an ETL (Extract, Transform, Load) workflow to process and store data in a database. It was designed to be interactive and user-friendly, ensuring that users could explore the data in an engaging manner while focusing on ethical data usage and interaction.

**Key Objectives and Tasks:**
**Data Visualization Track:**

**Visualization Creation:**

**Libraries Used:** The project utilized libraries like Python (Matplotlib, Pandas) and JavaScript (Plotly) to create interactive visualizations.
**Visualizations:** At least three unique views were implemented, presenting the data in a way that was digestible for users of all skill levels. The views allowed users to explore the data through:

**Interactive bar charts**
Line graphs or scatter plots for data trends
Bubble charts or heatmaps for detailed analysis
**Interactivity:** The visualizations were powered by HTML dropdowns, menus, and textboxes, providing users with the ability to filter and select data dynamically. The visualizations updated based on user-selected filters.

**Data Source and Database:**

**Dataset:** A dataset containing over 100 records was stored in a database (PostgreSQL, MongoDB, SQLite, etc.).
The project implemented a Flask backend to serve visualizations, fetch data from the database, and provide an API to handle user-driven requests.
User-driven Interaction:

The visualizations allowed users to filter and interact with the data, dynamically adjusting the visuals based on the selection.
JavaScript and Python libraries not covered in class were incorporated to enhance functionality.
Project Documentation:

A detailed README.md is provided, including:
An overview of the project and its purpose
Instructions for using the application and interacting with the visualizations
Ethical considerations and references to data sources and code used

**Data Engineering Track:**
**ETL Workflow:**

Data was processed using an ETL pipeline. The data was extracted from raw sources, transformed (e.g., cleaning, aggregating, or reformatting), and loaded into a structured database for future use.
Database Structure: The project included at least two tables or collections in the database, making it suitable for relational or NoSQL storage.
Data Transformation:

The raw dataset underwent transformations before being stored, ensuring it was properly cleaned and optimized for analysis.
Tools Used: Libraries related to data engineering, such as Pandas for data manipulation and other tools like SQLAlchemy for database interaction, were employed.
Database and API:

A Flask API was created to fetch and serve transformed data from the database, outputting results as JSON for further analysis or presentation.
Additional Library: A library not covered in class was incorporated to manage data pipelines, cloud integration, or streaming.
Project Documentation for Data Engineering:

**The README.md included:**
Details about the database used, and reasons for choosing a specific SQL or NoSQL database.
Diagrams and explanations of the ETL workflow.
Ethical considerations regarding data usage and potential biases.

**Group Collaboration and Final Presentation:**
**Collaborative Effort:**

Group members collaborated effectively by dividing tasks related to data collection, transformation, and visualization. Regular meetings ensured alignment and timely progress.
Group Presentation:

The project culminated in a presentation where each member contributed, explaining the development process, visualizations, and insights gained from the data.
The presentation was designed to be engaging and informative, ensuring the audience could follow the story told through the data.
GitHub Repository:

The final version of the project was hosted on GitHub, including all relevant code, visualizations, and the database. It was deployed and accessible for the review and usage of external users.
**Outcome:**
The project successfully met both the Data Visualization and Data Engineering track requirements.
The visualizations presented the data in a clear and insightful manner, with user interaction to explore different views and metrics.
The ETL process transformed raw data into a structured format, stored in a robust database, and served back through a Flask API.
Ethical considerations, including data privacy, fairness, and transparency, were taken into account during the development process.
The final product, including the presentation and deployment, demonstrated a comprehensive understanding of both data analysis and engineering principles.
The project effectively showcased the ability to manipulate large datasets, create interactive visualizations, and design workflows for efficient data management, making it a complete and polished solution for the chosen domain.
